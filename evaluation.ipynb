{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c1c6290",
   "metadata": {},
   "source": [
    "# Meme Retrieval Evaluation\n",
    "This notebook implements IR metrics (MRR, Precision@K, Recall@K, mAP) for text-to-image retrieval, and provides analysis and visualization including Precision-Recall curves and MRR breakdowns by meme category and query length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4588d88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "query",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "directory",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "697fccb1-1cb7-4fd3-b66b-fc36ce570ff5",
       "rows": [
        [
         "0",
         "student life memes",
         "2.0",
         "test_images/meme_submissions_1490490.png"
        ],
        [
         "1",
         "final exam memes",
         "2.0",
         "test_images/sad-baby_92.png"
        ],
        [
         "2",
         "data science memes",
         "2.0",
         "test_images/John_Daly_and_Tiger_Woods_1.png"
        ],
        [
         "3",
         "machine learning memes",
         "1.0",
         "test_images/batman-and-superman_20.png"
        ],
        [
         "4",
         "math major memes",
         "2.0",
         "test_images/big-book-small-book_21.png"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>label</th>\n",
       "      <th>directory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>student life memes</td>\n",
       "      <td>2.0</td>\n",
       "      <td>test_images/meme_submissions_1490490.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>final exam memes</td>\n",
       "      <td>2.0</td>\n",
       "      <td>test_images/sad-baby_92.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data science memes</td>\n",
       "      <td>2.0</td>\n",
       "      <td>test_images/John_Daly_and_Tiger_Woods_1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>machine learning memes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>test_images/batman-and-superman_20.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>math major memes</td>\n",
       "      <td>2.0</td>\n",
       "      <td>test_images/big-book-small-book_21.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    query  label                                    directory\n",
       "0      student life memes    2.0     test_images/meme_submissions_1490490.png\n",
       "1        final exam memes    2.0                  test_images/sad-baby_92.png\n",
       "2      data science memes    2.0  test_images/John_Daly_and_Tiger_Woods_1.png\n",
       "3  machine learning memes    1.0       test_images/batman-and-superman_20.png\n",
       "4        math major memes    2.0       test_images/big-book-small-book_21.png"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# Load test data\n",
    "df_test_meme = pd.read_csv('test_meme.csv', names=['query', 'label','directory'])\n",
    "df_test_template = pd.read_csv('test_template.csv', names=['query', 'label','directory'])\n",
    "df_test_meme = df_test_meme.dropna()\n",
    "df_test_template = df_test_template.dropna()\n",
    "df_test_meme.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61c36658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. IR Metrics ---\n",
    "def mean_reciprocal_rank(rs):\n",
    "    return np.mean([1/(np.where(r)[0][0]+1) if np.any(r) else 0 for r in rs])\n",
    "\n",
    "def precision_at_k(r, k):\n",
    "    r = np.asarray(r)[:k]\n",
    "    return np.mean(r)\n",
    "\n",
    "def recall_at_k(r, k, total_relevant):\n",
    "    r = np.asarray(r)[:k]\n",
    "    return np.sum(r) / total_relevant if total_relevant else 0\n",
    "\n",
    "def average_precision(r):\n",
    "    r = np.asarray(r)\n",
    "    out = [precision_at_k(r, k+1) for k in range(len(r)) if r[k]]\n",
    "    return np.mean(out) if out else 0\n",
    "\n",
    "def mean_average_precision(rs):\n",
    "    return np.mean([average_precision(r) for r in rs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c13899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Build Relevance Judgments ---\n",
    "def build_relevance_judgments(df):\n",
    "    rel = defaultdict(dict)\n",
    "    for _, row in df.iterrows():\n",
    "        q = row['query']\n",
    "        d = row['directory']\n",
    "        l = row['label']\n",
    "        rel[q][d] = int(l)\n",
    "    return rel\n",
    "\n",
    "rel_meme = build_relevance_judgments(df_test_meme)\n",
    "rel_template = build_relevance_judgments(df_test_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "677a0c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Connect to your model for retrieval ---\n",
    "from frontend import main  # main(query) returns image path for a given query\n",
    "\n",
    "def model_retrieve_text_to_image(query, candidates):\n",
    "    # Use your model to get the top_k image paths for a text query\n",
    "    # Here, we assume main(query) returns the best image path; you may want to adapt for top_k\n",
    "    result = main(query)\n",
    "    # If your model supports batch or top-k, replace this logic accordingly\n",
    "    if result in candidates:\n",
    "        ranked = [result] + [c for c in candidates if c != result]\n",
    "    else:\n",
    "        ranked = candidates[:]\n",
    "    return ranked\n",
    "\n",
    "# --- Use these in your evaluation functions ---\n",
    "def evaluate_text_to_image_with_model(rel_judgments, k=5):\n",
    "    queries = list(rel_judgments.keys())\n",
    "    all_dirs = set(d for q in rel_judgments for d in rel_judgments[q])\n",
    "    rs = []\n",
    "    for q in queries:\n",
    "        ranked = model_retrieve_text_to_image(q, list(all_dirs))\n",
    "        rels = [rel_judgments[q].get(d, 0) for d in ranked]\n",
    "        binary_rels = [1 if r == 2 else 0 for r in rels]\n",
    "        rs.append(binary_rels)\n",
    "    print(\"Text-to-Image (Model):\")\n",
    "    print(\"MRR:\", mean_reciprocal_rank(rs))\n",
    "    print(\"mAP:\", mean_average_precision(rs))\n",
    "    print(\"Precision@K:\", np.mean([precision_at_k(r, k) for r in rs]))\n",
    "    print(\"Recall@K:\", np.mean([recall_at_k(r, k, sum(1 for v in r if v == 1)) for r in rs]))\n",
    "    return rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0ac1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_text_to_image_with_model(rel_meme)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4874c35",
   "metadata": {},
   "source": [
    "## Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a4411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Precision-Recall Curve ---\n",
    "def plot_pr_curve_for_query(rel_judgments, query, ranked_dirs):\n",
    "    y_true = [1 if rel_judgments[query].get(d, 0) == 2 else 0 for d in ranked_dirs]\n",
    "    y_scores = list(reversed(range(len(ranked_dirs))))  # Simulate scores (replace with model scores)\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    ap = average_precision_score(y_true, y_scores)\n",
    "    plt.plot(recall, precision, marker='.')\n",
    "    plt.title(f'PR Curve for \"{query}\" (AP={ap:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b3781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
