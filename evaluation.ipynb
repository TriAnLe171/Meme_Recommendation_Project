{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95460c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. IR Metrics ---\n",
    "\n",
    "def mean_reciprocal_rank(rs):\n",
    "    \"\"\"rs: list of lists of binary relevance (1 or 0) for each query\"\"\"\n",
    "    return np.mean([1/(np.where(r)[0][0]+1) if np.any(r) else 0 for r in rs])\n",
    "\n",
    "def precision_at_k(r, k):\n",
    "    \"\"\"r: binary relevance list, k: int\"\"\"\n",
    "    r = np.asarray(r)[:k]\n",
    "    return np.mean(r)\n",
    "\n",
    "def recall_at_k(r, k, total_relevant):\n",
    "    \"\"\"r: binary relevance list, k: int, total_relevant: int\"\"\"\n",
    "    r = np.asarray(r)[:k]\n",
    "    return np.sum(r) / total_relevant if total_relevant else 0\n",
    "\n",
    "def average_precision(r):\n",
    "    \"\"\"r: binary relevance list\"\"\"\n",
    "    r = np.asarray(r)\n",
    "    out = [precision_at_k(r, k+1) for k in range(len(r)) if r[k]]\n",
    "    return np.mean(out) if out else 0\n",
    "\n",
    "def mean_average_precision(rs):\n",
    "    \"\"\"rs: list of binary relevance lists\"\"\"\n",
    "    return np.mean([average_precision(r) for r in rs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeaf1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, queries, relevance_judgments, k=5):\n",
    "    all_precisions, all_recalls, all_ap, all_rr = [], [], [], []\n",
    "    for q in queries:\n",
    "        # ranked_meme_ids = model.retrieve(q)  # Implement your retrieval\n",
    "        ranked_meme_ids = []  # Placeholder\n",
    "        rels = [relevance_judgments.get(q, {}).get(mid, 0) for mid in ranked_meme_ids]\n",
    "        binary_rels = [1 if r == 2 else 0 for r in rels]  # Only 'relevant' counts as 1\n",
    "        total_relevant = sum(1 for v in relevance_judgments.get(q, {}).values() if v == 2)\n",
    "        all_precisions.append(precision_at_k(binary_rels, k))\n",
    "        all_recalls.append(recall_at_k(binary_rels, k, total_relevant))\n",
    "        all_ap.append(average_precision(binary_rels))\n",
    "        all_rr.append(1/(np.where(np.array(binary_rels)==1)[0][0]+1) if 1 in binary_rels else 0)\n",
    "    return {\n",
    "        \"Precision@K\": np.mean(all_precisions),\n",
    "        \"Recall@K\": np.mean(all_recalls),\n",
    "        \"mAP\": np.mean(all_ap),\n",
    "        \"MRR\": np.mean(all_rr)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f8475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, marker='o')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c4f508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
